{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e675533d",
   "metadata": {},
   "source": [
    "# ü§ñ Hand Gesture Recognition System\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d847d",
   "metadata": {},
   "source": [
    "## 1. üì¶ Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nbimporter\n",
    "from data_preprocessing import DATA_DIR\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Input, GlobalAveragePooling2D ,BatchNormalization ,Flatten ,Bidirectional , GRU , Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger ,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2652b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. üóÇÔ∏è Load Frame Sequences and Create TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7248f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence_frames(sample_path, seq_len=15, step = 2, size=(224, 224)):\n",
    "    frames = []\n",
    "    image_files = sorted(sample_path.glob('*.png'))\n",
    "    selected_files = image_files[::step][:seq_len]\n",
    "\n",
    "    for img_path in selected_files:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, size)\n",
    "        frames.append(img)\n",
    "\n",
    "    frames = np.array(frames, dtype=np.float32) / 255.0\n",
    "    return frames\n",
    "\n",
    "def data_generator(root_dir):\n",
    "    root = Path(root_dir)\n",
    "    classes = sorted([d.name for d in root.iterdir() if d.is_dir()])\n",
    "    print(classes)\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    samples, labels = [], []\n",
    "    for c in classes:\n",
    "        for sample_folder in (root / c).iterdir():\n",
    "            if sample_folder.is_dir():\n",
    "                samples.append(sample_folder)\n",
    "                labels.append(class_to_idx[c])\n",
    "\n",
    "    def gen():\n",
    "        for sample_path, label in zip(samples, labels):\n",
    "            seq = load_sequence_frames(sample_path)\n",
    "            yield seq, label\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(15, 224, 224, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c2314",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. üìä Create Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_generator(f\"{DATA_DIR}/train\")\\\n",
    "    .shuffle(100)\\\n",
    "    .batch(4)\\\n",
    "    .repeat()\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = data_generator(f\"{DATA_DIR}/val\")\\\n",
    "    .batch(4)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ae0a8",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. üß† Build the MobileNetV2 + Bidirectional GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525abddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "sequence_length = 15\n",
    "image_size = (224, 224, 3)\n",
    "\n",
    "inputs = Input(shape=(sequence_length, *image_size))\n",
    "cnn_base = MobileNetV2(include_top=False, weights='imagenet', input_shape=image_size)\n",
    "cnn_base.trainable = True\n",
    "x = TimeDistributed(cnn_base)(inputs)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "x = Bidirectional(GRU(128), kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb7ba5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. üî¢ Compute Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_dataset = data_generator(f\"{DATA_DIR}/train\")\n",
    "num_train_samples = sum(1 for _ in original_train_dataset)\n",
    "\n",
    "original_val_dataset = data_generator(f\"{DATA_DIR}/val\")\n",
    "num_val_samples = sum(1 for _ in original_val_dataset)\n",
    "\n",
    "steps_per_epoch = num_train_samples // 4\n",
    "validation_steps = num_val_samples // 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1603d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. üèãÔ∏è Train the Model with Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(f\"{DATA_DIR}/Outputs/best_model.keras\", save_best_only=True, monitor='val_accuracy')\n",
    "csv_logger = CSVLogger(f\"{DATA_DIR}/Outputs/training_log.csv\", append=False)\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=40,\n",
    "    callbacks=[early_stop, checkpoint , csv_logger , reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba8986",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. üìà Plot Training and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(f\"{DATA_DIR}/Outputs/training_log.csv\")\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history['val_accuracy'], label='Val Accuracy', marker='o')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
