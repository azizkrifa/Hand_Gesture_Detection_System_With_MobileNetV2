{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e675533d",
   "metadata": {},
   "source": [
    "# ü§ñ Hand Gesture Recognition System\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d847d",
   "metadata": {},
   "source": [
    "## 1. üì¶ Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nbimporter\n",
    "from data_preprocessing import DATA_DIR\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Input, GlobalAveragePooling2D, BatchNormalization, Flatten, Bidirectional, GRU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2652b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. üóÇÔ∏è Load Frame Sequences and Create TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7248f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sequence of image frames from a given folder\n",
    "def load_sequence_frames(sample_path, seq_len=15, step=2, size=(224, 224)):\n",
    "    frames = []\n",
    "\n",
    "    # Get all PNG files sorted alphabetically\n",
    "    image_files = sorted(sample_path.glob('*.png'))\n",
    "\n",
    "    # Select frames with a stride (e.g. every 2nd frame)\n",
    "    selected_files = image_files[::step][:seq_len]\n",
    "\n",
    "    for img_path in selected_files:\n",
    "        img = cv2.imread(str(img_path))                  # Read image\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)       # Convert BGR to RGB\n",
    "        img = cv2.resize(img, size)                      # Resize image to desired size\n",
    "        frames.append(img)                               # Append to list\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    frames = np.array(frames, dtype=np.float32) / 255.0\n",
    "    return frames  # Shape: (seq_len, 224, 224, 3)\n",
    "\n",
    "\n",
    "# Create a TensorFlow dataset from a root directory\n",
    "def data_generator(root_dir):\n",
    "    root = Path(root_dir)\n",
    "\n",
    "    # Get class names (subdirectories)\n",
    "    classes = sorted([d.name for d in root.iterdir() if d.is_dir()])\n",
    "    print(classes)\n",
    "\n",
    "    # Map class names to numeric labels\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    samples, labels = [], []\n",
    "\n",
    "    # Collect all sample folders and their labels\n",
    "    for c in classes:\n",
    "        for sample_folder in (root / c).iterdir():\n",
    "            if sample_folder.is_dir():\n",
    "                samples.append(sample_folder)\n",
    "                labels.append(class_to_idx[c])\n",
    "\n",
    "    # Generator function that yields (sequence, label) pairs\n",
    "    def gen():\n",
    "        for sample_path, label in zip(samples, labels):\n",
    "            seq = load_sequence_frames(sample_path)\n",
    "            yield seq, label\n",
    "\n",
    "    # Wrap the generator in a tf.data.Dataset\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(15, 224, 224, 3), dtype=tf.float32),  # Input shape\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)                    # Label\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c2314",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. üìä Create Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (data_generator(f\"{DATA_DIR}/train\")\n",
    "    .shuffle(100)\n",
    "    .batch(4)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = (data_generator(f\"{DATA_DIR}/val\")\n",
    "    .batch(4)\n",
    "    .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ae0a8",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. üß† Build the MobileNetV2 + Bidirectional GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525abddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5 \n",
    "sequence_length = 15 # Each input sample is a sequence of 15 frames (images)\n",
    "image_size = (224, 224, 3) # Size of each image frame (224x224 RGB)\n",
    "\n",
    "# Define model input: a sequence of 15 images\n",
    "inputs = Input(shape=(sequence_length, *image_size))\n",
    "\n",
    "# Load MobileNetV2 as the base CNN to extract features from each frame\n",
    "# - include_top=False: we remove the final classification layer since it doesn't match ours \n",
    "# - weights='imagenet': use pretrained weights\n",
    "cnn_base = MobileNetV2(include_top=False, weights='imagenet', input_shape=image_size)\n",
    "\n",
    "# Make CNN layers trainable (fine-tuning)\n",
    "cnn_base.trainable = True\n",
    "\n",
    "# Apply the CNN to each frame independently using TimeDistributed\n",
    "x = TimeDistributed(cnn_base)(inputs)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(GlobalAveragePooling2D())(x)  # Convert CNN output to 1D vector per frame\n",
    "\n",
    "# Pass the sequence of vectors to a Bidirectional GRU to learn temporal patterns\n",
    "x = Bidirectional(GRU(128), kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.25)(x)  # Regularization to prevent overfitting\n",
    "\n",
    "# Fully connected layer \n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "# Final output layer with softmax for multi-class classification\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model with Adam optimizer and cross-entropy loss\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='sparse_categorical_crossentropy',  # since labels are integers and not hot encode , we use sparse_categorical_crossentropy\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb7ba5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. üî¢ Compute Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_dataset = data_generator(f\"{DATA_DIR}/train\")\n",
    "num_train_samples = sum(1 for _ in original_train_dataset)\n",
    "\n",
    "original_val_dataset = data_generator(f\"{DATA_DIR}/val\")\n",
    "num_val_samples = sum(1 for _ in original_val_dataset)\n",
    "\n",
    "# Compute the number of steps per training epoch (batch size = 4)\n",
    "steps_per_epoch = num_train_samples // 4\n",
    "\n",
    "# Compute the number of validation steps per epoch (batch size = 4)\n",
    "validation_steps = num_val_samples // 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1603d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. üèãÔ∏è Train the Model with Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training early if validation loss doesn't improve for 10 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Save the model only when it achieves the best validation accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"{DATA_DIR}/Outputs/best_model.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_accuracy'\n",
    ")\n",
    "\n",
    "# Log training and validation metrics to a CSV file\n",
    "csv_logger = CSVLogger(f\"{DATA_DIR}/Outputs/training_log.csv\", append=False)\n",
    "\n",
    "# Reduce learning rate if validation loss stops improving for 3 epochs\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',   # Watch the validation loss\n",
    "    factor=0.5,           # Reduce LR by half\n",
    "    patience=3,           # Wait 3 epochs before reducing\n",
    "    verbose=1,            # Print info when LR is reduced\n",
    "    min_lr=1e-7           # Set a minimum bound for LR\n",
    ")\n",
    "\n",
    "# Train the model with the above callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=40,\n",
    "    callbacks=[early_stop, checkpoint, csv_logger, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba8986",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. üìà Plot Training and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(f\"{DATA_DIR}/Outputs/training_log.csv\")\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history['val_accuracy'], label='Val Accuracy', marker='o')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
